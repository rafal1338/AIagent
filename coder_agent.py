import os
# Importujemy load_dotenv do wczytania pliku .env
from dotenv import load_dotenv
from langchain_ollama import ChatOllama
from langgraph.prebuilt import create_react_agent
from langchain_core.messages import HumanMessage, SystemMessage
from tools import coder_tools

# --- 0. Åadowanie Konfiguracji ---
# Wczytuje zmienne z pliku .env do os.environ
load_dotenv()

# Pobieramy zmienne z bezpiecznymi wartoÅ›ciami domyÅ›lnymi
OLLAMA_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "qwen2-coder:30b")
OLLAMA_TOKEN = os.getenv("OLLAMA_TOKEN", "")
# Konwersja stringa z .env na boolean (np. "False" -> False)
SSL_VERIFY_STR = os.getenv("OLLAMA_VERIFY_SSL", "True")
SSL_VERIFY = SSL_VERIFY_STR.lower() in ('true', '1', 't')

# Sprawdzenie czy token zostaÅ‚ podany (dla bezpieczeÅ„stwa)
if not OLLAMA_TOKEN:
    print("âš ï¸ OSTRZEÅ»ENIE: Brak OLLAMA_TOKEN w pliku .env. Autoryzacja moÅ¼e siÄ™ nie powieÅ›Ä‡.")

# --- 1. Konfiguracja Modelu ---
llm = ChatOllama(
    model=OLLAMA_MODEL,
    base_url=OLLAMA_URL,
    temperature=0,
    # Przekazujemy argumenty do klienta HTTP (httpx)
    client_kwargs={
        "verify": SSL_VERIFY,  # UÅ¼ywa wartoÅ›ci z .env (False dla self-signed)
        "headers": {
            "Authorization": f"Bearer {OLLAMA_TOKEN}"  # Pobiera token z .env
        }
    }
)

# --- 2. Prompt Systemowy ---
SYSTEM_PROMPT = """JesteÅ› Ekspertem ProgramistÄ… (Coder Agent).
Twoim celem jest pisanie dziaÅ‚ajÄ…cego, czystego kodu Python na podstawie poleceÅ„.

ZASADY KRYTYCZNE:
1. JeÅ›li otrzymasz zadanie napisania kodu, MUSISZ go zapisaÄ‡ uÅ¼ywajÄ…c narzÄ™dzia 'write_code_file'.
2. JeÅ›li zadanie odwoÅ‚uje siÄ™ do specyfikacji, najpierw jÄ… przeczytaj uÅ¼ywajÄ…c 'read_project_spec'.
3. Nie pytaj uÅ¼ytkownika o zdanie. DziaÅ‚aj autonomicznie.
4. JeÅ›li napotkasz bÅ‚Ä…d podczas zapisu, sprÃ³buj ponownie.
5. ZAWSZE nadpisuj plik, jeÅ›li tworzysz nowÄ… wersjÄ™.
"""

# --- 3. Tworzenie Agenta (LangGraph) ---
agent_app = create_react_agent(llm, coder_tools)

def run_coder_agent(task: str):
    """
    Funkcja wrapper uruchamiajÄ…ca agenta LangGraph z podanym zadaniem.
    """
    print(f"ğŸš€ [LangGraph] Agent Programista otrzymaÅ‚ zadanie: {task}")
    print(f"   (Model: {OLLAMA_MODEL}, URL: {OLLAMA_URL}, SSL Verify: {SSL_VERIFY})")
    
    messages = [
        SystemMessage(content=SYSTEM_PROMPT),
        HumanMessage(content=task)
    ]
    
    inputs = {"messages": messages}
    
    try:
        # Uruchamiamy graf agenta
        result = agent_app.invoke(inputs)
        
        # Ostatnia wiadomoÅ›Ä‡ w historii to odpowiedÅº koÅ„cowa modelu
        last_message = result["messages"][-1]
        return {"output": last_message.content}
        
    except Exception as e:
        error_msg = f"âŒ BÅ‚Ä…d krytyczny Agenta LangGraph: {e}"
        print(error_msg)
        # JeÅ›li bÅ‚Ä…d dotyczy SSL, dajemy podpowiedÅº
        if "SSLError" in str(e):
            print("ğŸ’¡ PodpowiedÅº: SprawdÅº ustawienie OLLAMA_VERIFY_SSL w pliku .env")
        return {"output": error_msg}

# --- Testowanie ---
if __name__ == "__main__":
    print("Testowanie Agenta z konfiguracjÄ… ENV...")
    res = run_coder_agent("Napisz plik 'env_test.py' wypisujÄ…cy 'Konfiguracja dziaÅ‚a!'")
    print(res['output'])